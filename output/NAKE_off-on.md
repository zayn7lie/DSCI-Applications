### Train

```
Origin: 1000 = 20 * 50 -> 2000 = 40 * 50 

K-fold: 18 * 50 + 36 * 50 -> 4 * 50 

Epoch: 1 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.523962 MAX=0.823180 MIN=0.374697
- [18/18] Loss: AVG=0.330582 MAX=0.358576 MIN=0.298475
Epoch: 1 Loss: 0.427272187338935 MMD: 0.0 BCE: 0.427272187338935 

Epoch: 2 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.307783 MAX=0.343495 MIN=0.281006
- [18/18] Loss: AVG=0.325076 MAX=0.368177 MIN=0.291367
Epoch: 2 Loss: 0.3164294709761937 MMD: 0.0 BCE: 0.3164294709761937 

Epoch: 3 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.303632 MAX=0.356205 MIN=0.268771
- [18/18] Loss: AVG=0.313659 MAX=0.345641 MIN=0.293802
Epoch: 3 Loss: 0.3086455050441954 MMD: 0.0 BCE: 0.3086455050441954 

Epoch: 4 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.294472 MAX=0.342821 MIN=0.257337
- [18/18] Loss: AVG=0.310325 MAX=0.335832 MIN=0.281459
Epoch: 4 Loss: 0.3023984399106767 MMD: 0.0 BCE: 0.3023984399106767 

Epoch: 5 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.298125 MAX=0.330572 MIN=0.241607
- [18/18] Loss: AVG=0.301711 MAX=0.360855 MIN=0.253586
Epoch: 5 Loss: 0.2999182823631499 MMD: 0.0 BCE: 0.2999182823631499 

Epoch: 6 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.295432 MAX=0.311463 MIN=0.264462
- [18/18] Loss: AVG=0.298275 MAX=0.337221 MIN=0.264561
Epoch: 6 Loss: 0.29685333536730873 MMD: 0.0 BCE: 0.29685333536730873 

Epoch: 7 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.275275 MAX=0.310717 MIN=0.230285
- [18/18] Loss: AVG=0.297671 MAX=0.345289 MIN=0.268844
Epoch: 7 Loss: 0.2864729439218839 MMD: 0.0 BCE: 0.2864729439218839 

Epoch: 8 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.279167 MAX=0.294804 MIN=0.241142
- [18/18] Loss: AVG=0.285290 MAX=0.317481 MIN=0.272863
Epoch: 8 Loss: 0.2822285029623244 MMD: 0.0 BCE: 0.2822285029623244 

Epoch: 9 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.269563 MAX=0.296704 MIN=0.227846
- [18/18] Loss: AVG=0.287487 MAX=0.312428 MIN=0.257004
Epoch: 9 Loss: 0.27852506107754177 MMD: 0.0 BCE: 0.27852506107754177 

Epoch: 10 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.266318 MAX=0.287954 MIN=0.231677
- [18/18] Loss: AVG=0.288034 MAX=0.312988 MIN=0.258949
Epoch: 10 Loss: 0.2771761616071065 MMD: 0.0 BCE: 0.2771761616071065 

Epoch: 11 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.259939 MAX=0.295458 MIN=0.222432
- [18/18] Loss: AVG=0.279526 MAX=0.330769 MIN=0.249172
Epoch: 11 Loss: 0.26973240988122094 MMD: 0.0 BCE: 0.26973240988122094 

Epoch: 12 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.259645 MAX=0.276506 MIN=0.240878
- [18/18] Loss: AVG=0.267462 MAX=0.308366 MIN=0.236164
Epoch: 12 Loss: 0.2635531731777721 MMD: 0.0 BCE: 0.2635531731777721 

Epoch: 13 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.246690 MAX=0.280367 MIN=0.194671
- [18/18] Loss: AVG=0.269924 MAX=0.311977 MIN=0.215614
Epoch: 13 Loss: 0.25830667714277905 MMD: 0.0 BCE: 0.25830667714277905 

Epoch: 14 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.231776 MAX=0.256999 MIN=0.185355
- [18/18] Loss: AVG=0.257492 MAX=0.295575 MIN=0.213910
Epoch: 14 Loss: 0.24463424334923425 MMD: 0.0 BCE: 0.24463424334923425 

Epoch: 15 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.232556 MAX=0.250878 MIN=0.213970
- [18/18] Loss: AVG=0.235187 MAX=0.266080 MIN=0.209930
Epoch: 15 Loss: 0.2338718209001753 MMD: 0.0 BCE: 0.2338718209001753 

Epoch: 16 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.239310 MAX=0.309661 MIN=0.187081
- [18/18] Loss: AVG=0.238408 MAX=0.261711 MIN=0.207848
Epoch: 16 Loss: 0.23885930081208548 MMD: 0.0 BCE: 0.23885930081208548 

Epoch: 17 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.223026 MAX=0.249118 MIN=0.186916
- [18/18] Loss: AVG=0.223133 MAX=0.270785 MIN=0.195364
Epoch: 17 Loss: 0.2230795423189799 MMD: 0.0 BCE: 0.2230795423189799 

Epoch: 18 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.211399 MAX=0.261256 MIN=0.165498
- [18/18] Loss: AVG=0.220929 MAX=0.270429 MIN=0.180796
Epoch: 18 Loss: 0.21616390844186148 MMD: 0.0 BCE: 0.21616390844186148 

Epoch: 19 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.204907 MAX=0.260256 MIN=0.175918
- [18/18] Loss: AVG=0.234886 MAX=0.308124 MIN=0.197933
Epoch: 19 Loss: 0.21989647299051285 MMD: 0.0 BCE: 0.21989647299051285 

Epoch: 20 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.222802 MAX=0.252656 MIN=0.179612
- [18/18] Loss: AVG=0.224591 MAX=0.259977 MIN=0.182523
Epoch: 20 Loss: 0.22369678649637434 MMD: 0.0 BCE: 0.22369678649637434 

Epoch: 21 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.216039 MAX=0.296286 MIN=0.179809
- [18/18] Loss: AVG=0.201700 MAX=0.236323 MIN=0.156055
Epoch: 21 Loss: 0.20886953175067902 MMD: 0.0 BCE: 0.20886953175067902 

Epoch: 22 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.180907 MAX=0.193839 MIN=0.165371
- [18/18] Loss: AVG=0.188313 MAX=0.223289 MIN=0.166217
Epoch: 22 Loss: 0.1846101035674413 MMD: 0.0 BCE: 0.1846101035674413 

Epoch: 23 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.187502 MAX=0.253354 MIN=0.148243
- [18/18] Loss: AVG=0.196628 MAX=0.247788 MIN=0.162464
Epoch: 23 Loss: 0.19206467933124965 MMD: 0.0 BCE: 0.19206467933124965 

Epoch: 24 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.168280 MAX=0.208866 MIN=0.132537
- [18/18] Loss: AVG=0.182322 MAX=0.237643 MIN=0.140095
Epoch: 24 Loss: 0.17530132002300686 MMD: 0.0 BCE: 0.17530132002300686 

Epoch: 25 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.145990 MAX=0.198548 MIN=0.114073
- [18/18] Loss: AVG=0.164519 MAX=0.195320 MIN=0.145445
Epoch: 25 Loss: 0.15525493315524524 MMD: 0.0 BCE: 0.15525493315524524 

Epoch: 26 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.147104 MAX=0.241155 MIN=0.113317
- [18/18] Loss: AVG=0.161423 MAX=0.197959 MIN=0.105406
Epoch: 26 Loss: 0.15426306799054146 MMD: 0.0 BCE: 0.15426306799054146 

Epoch: 27 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.141314 MAX=0.196502 MIN=0.086800
- [18/18] Loss: AVG=0.163630 MAX=0.209307 MIN=0.123932
Epoch: 27 Loss: 0.1524722302953402 MMD: 0.0 BCE: 0.1524722302953402 

Epoch: 28 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.131314 MAX=0.182015 MIN=0.111403
- [18/18] Loss: AVG=0.149976 MAX=0.197802 MIN=0.116717
Epoch: 28 Loss: 0.14064493733975622 MMD: 0.0 BCE: 0.14064493733975622 

Epoch: 29 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.122247 MAX=0.155245 MIN=0.097508
- [18/18] Loss: AVG=0.122545 MAX=0.155178 MIN=0.082332
Epoch: 29 Loss: 0.12239631141225497 MMD: 0.0 BCE: 0.12239631141225497 

Epoch: 30 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.097239 MAX=0.115175 MIN=0.076188
- [18/18] Loss: AVG=0.089766 MAX=0.112919 MIN=0.059371
Epoch: 30 Loss: 0.0935024679121044 MMD: 0.0 BCE: 0.0935024679121044 

Epoch: 31 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.077956 MAX=0.096606 MIN=0.056933
- [18/18] Loss: AVG=0.098249 MAX=0.115194 MIN=0.053214
Epoch: 31 Loss: 0.08810277117623223 MMD: 0.0 BCE: 0.08810277117623223 

Epoch: 32 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.083473 MAX=0.146669 MIN=0.052175
- [18/18] Loss: AVG=0.092792 MAX=0.130092 MIN=0.070678
Epoch: 32 Loss: 0.08813274506893423 MMD: 0.0 BCE: 0.08813274506893423 

Epoch: 33 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.084042 MAX=0.137852 MIN=0.047339
- [18/18] Loss: AVG=0.088112 MAX=0.134487 MIN=0.057372
Epoch: 33 Loss: 0.08607671927246782 MMD: 0.0 BCE: 0.08607671927246782 

Epoch: 34 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.063341 MAX=0.117082 MIN=0.044779
- [18/18] Loss: AVG=0.066542 MAX=0.110649 MIN=0.037041
Epoch: 34 Loss: 0.06494155360592736 MMD: 0.0 BCE: 0.06494155360592736 

Epoch: 35 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.063334 MAX=0.106736 MIN=0.028666
- [18/18] Loss: AVG=0.074389 MAX=0.143490 MIN=0.038460
Epoch: 35 Loss: 0.06886172698189814 MMD: 0.0 BCE: 0.06886172698189814 

Epoch: 36 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.052160 MAX=0.067833 MIN=0.030057
- [18/18] Loss: AVG=0.071740 MAX=0.093702 MIN=0.055395
Epoch: 36 Loss: 0.061949950435923204 MMD: 0.0 BCE: 0.061949950435923204 

Epoch: 37 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.061051 MAX=0.121420 MIN=0.033373
- [18/18] Loss: AVG=0.072409 MAX=0.128988 MIN=0.045596
Epoch: 37 Loss: 0.06673024139470524 MMD: 0.0 BCE: 0.06673024139470524 

Epoch: 38 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.067822 MAX=0.115554 MIN=0.036240
- [18/18] Loss: AVG=0.068925 MAX=0.101803 MIN=0.038649
Epoch: 38 Loss: 0.06837371695372793 MMD: 0.0 BCE: 0.06837371695372793 

Epoch: 39 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.043341 MAX=0.067901 MIN=0.026454
- [18/18] Loss: AVG=0.045894 MAX=0.061208 MIN=0.032319
Epoch: 39 Loss: 0.04461748887681299 MMD: 0.0 BCE: 0.04461748887681299 

Epoch: 40 Learning Rate: 0.0001
- [9/18] Loss: AVG=0.035229 MAX=0.071499 MIN=0.022440
- [18/18] Loss: AVG=0.035872 MAX=0.091407 MIN=0.017501
Epoch: 40 Loss: 0.03555063178969754 MMD: 0.0 BCE: 0.03555063178969754 

Epoch: 41 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.026168 MAX=0.038829 MIN=0.015648
- [18/18] Loss: AVG=0.020187 MAX=0.034185 MIN=0.008457
Epoch: 41 Loss: 0.02317730513297849 MMD: 0.0 BCE: 0.02317730513297849 

Epoch: 42 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.026497 MAX=0.052188 MIN=0.011992
- [18/18] Loss: AVG=0.024969 MAX=0.053295 MIN=0.014305
Epoch: 42 Loss: 0.02573302641717924 MMD: 0.0 BCE: 0.02573302641717924 

Epoch: 43 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.021027 MAX=0.043162 MIN=0.011828
- [18/18] Loss: AVG=0.015329 MAX=0.025389 MIN=0.005267
Epoch: 43 Loss: 0.018177660492559273 MMD: 0.0 BCE: 0.018177660492559273 

Epoch: 44 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.016399 MAX=0.029739 MIN=0.008160
- [18/18] Loss: AVG=0.020421 MAX=0.073697 MIN=0.008338
Epoch: 44 Loss: 0.0184100818199416 MMD: 0.0 BCE: 0.0184100818199416 

Epoch: 45 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.019883 MAX=0.048817 MIN=0.004604
- [18/18] Loss: AVG=0.013857 MAX=0.018677 MIN=0.008279
Epoch: 45 Loss: 0.01687000681542688 MMD: 0.0 BCE: 0.01687000681542688 

Epoch: 46 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.016969 MAX=0.035722 MIN=0.005910
- [18/18] Loss: AVG=0.021760 MAX=0.054826 MIN=0.007001
Epoch: 46 Loss: 0.019364437657511897 MMD: 0.0 BCE: 0.019364437657511897 

Epoch: 47 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.010265 MAX=0.016746 MIN=0.004668
- [18/18] Loss: AVG=0.011176 MAX=0.019389 MIN=0.005106
Epoch: 47 Loss: 0.010720363119617105 MMD: 0.0 BCE: 0.010720363119617105 

Epoch: 48 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.012046 MAX=0.023319 MIN=0.007963
- [18/18] Loss: AVG=0.007846 MAX=0.017409 MIN=0.005041
Epoch: 48 Loss: 0.00994585219046308 MMD: 0.0 BCE: 0.00994585219046308 

Epoch: 49 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.013746 MAX=0.032905 MIN=0.005650
- [18/18] Loss: AVG=0.015605 MAX=0.037579 MIN=0.006284
Epoch: 49 Loss: 0.01467541499166853 MMD: 0.0 BCE: 0.01467541499166853 

Epoch: 50 Learning Rate: 1e-05
- [9/18] Loss: AVG=0.009325 MAX=0.015210 MIN=0.004170
- [18/18] Loss: AVG=0.011509 MAX=0.028021 MIN=0.004544
Epoch: 50 Loss: 0.010417095178531276 MMD: 0.0 BCE: 0.010417095178531276 
```

### Test

```
0 0.3695652173913044 0.6255599472990777
0 0.7291157791890571 1.2546372192334925
0 0.9987787005373717 1.8332626338384623
0 1.254592654025744 2.406721816938637

Val set: BCE: 0.5798 Average f1: 31.3648% Average auc: 60.1680%
```